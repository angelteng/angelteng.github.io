<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/blog/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/blog/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/blog/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Code war of Angel">
<meta property="og:url" content="https://angelteng.github.io/blog/page/2/index.html">
<meta property="og:site_name" content="Code war of Angel">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Code war of Angel">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://angelteng.github.io/blog/page/2/">





  <title>Code war of Angel</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Code war of Angel</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://angelteng.github.io/blog/blog/2019/05/29/支持向量机SVM/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Angel Teng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://angelteng.github.io/blog/images/angel.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Code war of Angel">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/blog/2019/05/29/支持向量机SVM/" itemprop="url">支持向量机SVM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-29T14:46:23+08:00">
                2019-05-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>支持向量机算法功能</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">有监督学习</td>
<td style="text-align:left">线性二分类与多分类(Linear Support Vector Classification) <br> 非线性二分类与多分类(Support Vector Classification, SVC)  <br> 普通连续型变量的回归(Support Vector Regression) 概率型连续变量的回归(Bayesian SVM)</td>
</tr>
<tr>
<td style="text-align:left">无监督学习</td>
<td style="text-align:left">支持向量聚类(Support Vector Clustering，SVC) 异常值检测(One-class SVM)</td>
</tr>
<tr>
<td style="text-align:left">半监督学习</td>
<td style="text-align:left">转导支持向量机(Transductive Support Vector Machines，TSVM)</td>
</tr>
</tbody>
</table>
<p>从分类效力来讲，SVM在无论线性还是非线性分类中，都是明星般的存在。</p>
<ol start="2">
<li><p>支持向量机的分类方法，是在这组分布中找出一个超平面作为决策边界，使模型在数据上的分类误差尽量接近于小，尤其是在未知数据集上的分类误差(泛化误差)尽量小。决策边界一侧的所有点在分类为属于一个类，而另一侧的所有点分类属于另一个类。如果我们能够找出决策边界， 分类问题就可以变成探讨每个样本对于决策边界而言的相对位置，因此，支持向量机，就是通过找出边际最大的决策边界，来对数据进行分类的分类器。</p>
</li>
<li><p>in sklearn<br><img src="0.png" alt="image.png"></p>
</li>
</ol>
<p>svm.SVC(C=1.0,kernel=’rbf’,degree=3,gamma=’auto_deprecated’,coef0=0.0,shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)</p>
<p>参数</p>
<pre><code>- kernel
    - linear
    - poly
    - sigmoid
    - rbf (good)
- degree
- gamma
</code></pre><p><img src="1.png" alt="image.png"><br><img src="2.png" alt="image.png"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.svm import SVC</span><br><span class="line">clf = SVC(kernel = &quot;linear&quot;).fit(X,y)</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>核函数<br>也叫做“核技巧”(Kernel Trick)，是一种能够使用数据原始空间中的向量计算来表示 升维后的空间中的点积结果的数学方式。 这个原始空间中的点积函数就被叫做“核函数”(Kernel Function)。<br>第一，有了核函数之后，我们无需去担心函数究竟应该是什么样，因为非线性SVM中的核函数都是正定核函数 (positive definite kernel functions)，他们都满足美世定律(Mercer’s theorem)，确保了高维空间中任意两个向量 的点积一定可以被低维空间中的这两个向量的某种计算来表示(多数时候是点积的某种变换)。<br>第二，使用核函数计算低维度中的向量关系比计算原本的要简单太多了。<br>第三，因为计算是在原始空间中进行，所以避免了维度诅咒的问题。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://angelteng.github.io/blog/blog/2019/05/29/聚类算法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Angel Teng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://angelteng.github.io/blog/images/angel.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Code war of Angel">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/blog/2019/05/29/聚类算法/" itemprop="url">聚类算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-29T14:23:16+08:00">
                2019-05-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>聚类算法又叫做“无监督分类”，其目的是将数据划分成有意义或有用的组(或簇)。这种划分可以基于我们的业务 需求或建模需求来完成，也可以单纯地帮助我们探索数据的自然结构和分布。</li>
<li><p>in sklearn<br><img src="0.png" alt="image.png"></p>
</li>
<li><p>KMeans聚类算法<br> KMeans算法将一组N个样本的特征矩阵X划分为K个无交集的簇，直观上来看是簇是一组一组聚集在一起的数据，在一个簇中的数据就认为是同一类。簇就是聚类的结果表现。<br> 簇中所有数据的均值通常被称为这个簇的“质心”(centroids)。在一个二维平面中，一簇数据点的质心的 横坐标就是这一簇数据点的横坐标的均值，质心的纵坐标就是这一簇数据点的纵坐标的均值。同理可推广至高维空间。<br> 流程：</p>
<pre><code>- 随机抽取K个样本作为最初的质心
- 开始循环:
- 将每个样本点分配到离他们最近的质心，生成K个簇
- 对于每个簇，计算所有被分到该簇的样本点的平均值作为新的质心
- 当质心的位置不再发生变化，迭代停止，聚类完成
</code></pre></li>
<li><p>簇内平方和（Inertia）：<br> Total Inertia越小，代表每个簇内样本越相似，聚类的效果就越好。因此 KMeans追求的是，求解能够让Inertia最小化的质心。<br> Inertia是基于欧几里得距离的计算公式得到的。<br> 簇内平方和/整体平方和是KMeans的损失函数。</p>
</li>
<li><p>cluster.KMeans (n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, tol=0.0001, precompute_distances=’auto’, verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm=’auto’)</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">n_clusters = 3</span><br><span class="line"># kmean不需要建立模型或者预测数据，因此只要fit就可以查看结果</span><br><span class="line">cluster = KMeans(n_clusters=n_clusters, random_state=0).fit(X)</span><br><span class="line"># 查看聚好的类别，每个样本对应的类</span><br><span class="line">y_pred = cluster.labels_</span><br><span class="line"># 查看质心</span><br><span class="line">centroid = cluster.cluster_centers_</span><br><span class="line"># 查看总距离平方和</span><br><span class="line">inertia = cluster.inertia_</span><br></pre></td></tr></table></figure>
<p> 参数：</p>
<pre><code>- n_clusters：分的类数
</code></pre></li>
<li><p>模型评估指标<br> 通过衡量簇内差异来衡量聚类的效果，可以通过轮廓系数来判断。</p>
<ul>
<li>样本与其自身所在的簇中的其他样本的相似度a，等于样本与同一簇中所有其他点之间的平均距离</li>
<li>样本与其他簇中的样本的相似度b，等于样本与下一个最近的簇中得所有点之间的平均距离<br>单个样本的轮廓系统为：s = (b-a) / max(b,a)<br>轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相 似度一致，两个簇本应该是一个簇。<br>缺点：再凸型函数表现虚高<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import silhouette_score</span><br><span class="line">from sklearn.metrics import silhouette_samples</span><br><span class="line">silhouette_score(X,cluster_.labels_) #总数</span><br><span class="line">silhouette_samples(X,y_pred) #每个样本</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://angelteng.github.io/blog/blog/2019/05/28/线性回归/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Angel Teng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://angelteng.github.io/blog/images/angel.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Code war of Angel">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/blog/2019/05/28/线性回归/" itemprop="url">线性回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-28T17:13:30+08:00">
                2019-05-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。</li>
<li><p>线性回归是机器学习中最简单的回归算法，多元线性回归指的就是一个样本有多个特征的线性回归问题。对于一个 有 个特征的样本 而言，它的回归结果可以写作一个几乎人人熟悉的方程: y = w0+w1x1+w2x2+w3x3…..+wnxn<br>预测函数的本质就是我们需要构建的模型，而构造预测函数的核心就是找出模型的参数向量。通过最小化损失函数或损失函数的某种变化来将求解参数向量，以此将单纯的求解问题转化为一个最优化问题。在多元线性回归中，我们的损失函数如下定义:<br><img src="0.png" alt="image.png"><br>在这个平方结果下，我们的y和y^分别是我们的真实标签和预测值，也就是说，这个损失函数实在计算我们的真实标 签和预测值之间的距离。因此，我们认为这个损失函数衡量了我们构造的模型的预测结果和真实标签的差异，因此我们固然希望我们的预测结果和真实值差异越小越好。所以我们的求解目标就可以转化成RSS残差平方和:<br><img src="1.png" alt="image.png"></p>
</li>
<li><p>最小二乘法：通过最小化真实值和预测值之间的RSS来求解参数的方法。<br><img src="2.png" alt="image.png"></p>
</li>
<li><p>linear_model.LinearRegression (fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)<br>参数：</p>
<ul>
<li>fit_intercept：是否计算此模型的截距，默认True</li>
<li>normalize： 当fit_intercept设置为False时，将忽略此参数。如果为True，则特征矩阵X在进入回归之前 将会被减去均值(中心化)并除以L2范式(缩放)。如果你希望进行标准化，请在fit数据 之前使用preprocessing模块中的标准化专用类StandardScaler。默认False</li>
<li>copy_X： 默认为True 如果为真，将在X.copy()上进行操作，否则的话原本的特征矩阵X可能被线性回归影响并覆盖。</li>
<li>n_jobs： 整数或者None，默认为None 用于计算的作业数。只在多标签的回归和数据量足够大的时候才生效。<br>属性：</li>
<li>coef_：线性回归方程中估计出的系数</li>
<li>intercept_： 线性回归中的截距<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression as LR</span><br><span class="line">reg = LR().fit(Xtrain, Ytrain)</span><br><span class="line">yhat = reg.predict(Xtest)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>评估</p>
<ol>
<li><p>是否预测了正确的值<br> 均方误差MSE：均方误差，本质是在RSS的基础上除以了样本总量，得到了每个样本量上的平均误差。<br> <img src="3.png" alt="image.png"></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import mean_squared_error as MSE</span><br><span class="line">MSE(y_true,y_predict)</span><br></pre></td></tr></table></figure>
<p> 再sklearn中，均方误差是个负数，代表一种损失。</p>
</li>
<li><p>是否拟合到了足够的信息</p>
<ul>
<li>R^2和</li>
<li><p>可解释性方差分数(explained_variance_score，EVS)<br><img src="4.png" alt="image.png"><br>两者都衡量 1 - 我们的模型没有捕获到的信息 量占真实标签中所带的信息量的比例，所以，两者都是越接近1越好。<br>注意方法参数传入的顺序。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import r2_score r2_score(yhat,Ytest)</span><br><span class="line">r2 = reg.score(Xtest,Ytest)</span><br><span class="line"></span><br><span class="line">from sklearn.metrics import explained_variance_score as EVS </span><br><span class="line">EVS(Ytest,yhat)</span><br></pre></td></tr></table></figure>
<p>R^2的值可能是个负数。证明模型拟合得非常糟糕</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://angelteng.github.io/blog/blog/2019/05/27/逻辑回归/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Angel Teng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://angelteng.github.io/blog/images/angel.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Code war of Angel">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/blog/2019/05/27/逻辑回归/" itemprop="url">逻辑回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-27T17:22:39+08:00">
                2019-05-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>逻辑回归，是一种名为“回归”的线性分类器，其本质是由线性回 归变化而来的，一种广泛使用于分类问题中的广义回归算法。</li>
<li>线性回归的任务，就是构造一个预测函数 来映射输入的特征矩阵x和标签值y的线性关系，而构造预测函数的核心 就是找出模型的参数<br><img src="0.png" alt="image.png"><br>通过引入联系函数(link function)，将线性回归方程z变换为g(z)，并且令g(z)的值 分布在(0,1)之间，且当g(z)接近0时样本的标签为类别0，当g(z)接近1时样本的标签为类别1，这样就得到了一个分 类模型。而这个联系函数对于逻辑回归来说，就是Sigmoid函数:<br><img src="1.png" alt="image.png"></li>
</ol>
<p>Sigmoid函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0，它能够将任何实数映射到(0,1)区间，使其可用于将任意值函数转换为更适合二分类的函数。 因为这个性质，Sigmoid函数也被当作是归一化的一种方法，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。区别在于，MinMaxScaler归一化之后，是可以取到0和1的(最大值归一化后就是1，最小值归一化后就是0)，但Sigmoid函数只是无限趋近于0和1。</p>
<p>g(z)的形似几率取对数的本质其实就是我们的线性回归z，我们实际上是在对线性回归模型的预测结果取 对数几率来让其的结果无限逼近0和1。因此，其对应的模型被称为”对数几率回归“(logistic Regression)，也就是逻辑回归。</p>
<ol start="3">
<li><p>优点：</p>
<ul>
<li>逻辑回归对线性关系的拟合效果好。</li>
<li>逻辑回归计算快</li>
<li>逻辑回归返回的分类结果不是固定的0，1，而是以小数形式呈现的类概率数字</li>
</ul>
</li>
<li><p><img src="2.png" alt="image.png"></p>
</li>
</ol>
<p>linear_model.LogisticRegression (penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression as LR</span><br><span class="line">lrl1 = LR(penalty=&quot;l1&quot;,solver=&quot;liblinear&quot;,C=0.5,max_iter=1000)</span><br><span class="line">lrl2 = LR(penalty=&quot;l2&quot;,solver=&quot;liblinear&quot;,C=0.5,max_iter=1000)</span><br></pre></td></tr></table></figure></p>
<p>参数：</p>
<pre><code>- penalty: &quot;l1&quot;/&quot;l2&quot;,指定使用哪一种正则化方式，不填写默认&quot;l2&quot;。 注意,若选择&quot;l1&quot;正则化，参数solver仅能够使用”liblinear&quot;，若使用“l2”正则化，参数solver中 所有的求解方式都可以使用。
- C: C正则化强度的倒数，必须是一个大于0的浮点数，不填写默认1.0，即默认一倍正则项。 C越小，对损失函数的惩罚越重，正则化的效力越强，参数会逐渐被压缩得越来越小。
- max_iter: 算法收敛最大迭代次数，int类型，默认为10。仅在正则化优化算法为newton-cg, sag和lbfgs才有用，算法收敛的最大迭代次数。
- solver: 优化算法选择参数，只有五个可选参数，即newton-cg,lbfgs,liblinear,sag,saga。默认为liblinear。solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是： 
    - liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。
    - lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
    - newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
    - sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。
    - saga：线性收敛的随机优化算法的的变重。
</code></pre><ol start="5">
<li><p>”损失函数“作为评估指标，来衡量参数的优劣，即这一组参数能否使模型在训练集上表现优异。即是说，我们在求解参数时，追求损失函数最小，让模型在训练数据上的拟合效果最优，即预测准确率尽量靠近100%。对逻辑回归中过拟合的控制，通过正则化实现。</p>
</li>
<li><p>正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向 量 的L1范式和L2范式的倍数来实现。这个增加的范式，被称为“正则项”，也被称为”惩罚项”。损失函数改变，基 于损失函数的最优化来求解的参数取值必然改变，我们以此来调节模型拟合的程度。其中L1范数表现为参数向量中 的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值。</p>
</li>
</ol>
<p><img src="3.png" alt="image.png"></p>
<p>L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大(即C逐渐变小)， 参数 的取值会逐渐变小，但L1正则化会将参数压缩为0，L2正则化只会让参数尽量小，不会取到0。</p>
<p>参考：<br><a href="https://blog.csdn.net/jark_/article/details/78342644" target="_blank" rel="noopener">LogisticRegression - 参数说明</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://angelteng.github.io/blog/blog/2019/05/27/降维算法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Angel Teng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://angelteng.github.io/blog/images/angel.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Code war of Angel">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/blog/2019/05/27/降维算法/" itemprop="url">降维算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-27T16:33:20+08:00">
                2019-05-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>降维算法中的”降维“，指的是降低特征矩阵中特征的数量。</li>
<li><p>in sklearn<br><img src="0.png" alt="image.png"></p>
</li>
<li><p>PCA使用的信息量衡量指标，就是样本方差，又称可解释性方差，方差越大，特征所带的信息量越多。<br><img src="1.png" alt="image.png"><br>Var代表一个特征的方差，n代表样本量，xi代表一个特征中的每个样本取值，xhat代表这一列样本的均值。</p>
<ul>
<li><a href="https://www.zhihu.com/question/20099757" target="_blank" rel="noopener">为什么方差的分母是n-1？</a></li>
</ul>
</li>
<li><p>矩阵分解：让数据能够被压缩到少数特征上并且总信息量不损失太多的技术。PCA和SVD是两种不同的降维算法，只是两种算法中矩阵分解的方法不同，信息量的衡量指标不同罢了。降维完成之后，PCA找到的每个新特征向量就叫做“主成分”，而被丢弃的特征 向量被认为信息量很少，这些信息很可能就是噪音。</p>
<ul>
<li>PCA使用方差作为信息量的衡量指标，并且特征值分解来找出空间V。</li>
<li>SVD使用奇异值分解来找出空间V</li>
</ul>
</li>
<li><p>降维与特征工程的区别：</p>
<ul>
<li>特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，我们依然知道这个特征在原数据的哪个位置，代表着原数据上的什么含义。</li>
<li>降维算法，是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某些方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓降维算法们都建立了怎样 的新特征向量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而来，新特征虽然带有原始数据的信息，却已经不是原数据上代表着的含义了。降维算法因此是特征创造(feature creation，或feature construction)的一种。</li>
<li>可以想见，PCA一般不适用于探索特征和标签之间的关系的模型(如线性回归)，因为无法解释的新特征和标签之间的关系不具有意义。在线性回归模型中，我们使用特征选择。</li>
</ul>
</li>
<li><p>sklearn.decomposition.PCA (n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0， iterated_power=’auto’, random_state=None)</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">#调用PCA</span><br><span class="line">pca = PCA(n_components=2) pca = pca.fit(X)</span><br><span class="line">X_dr = pca.transform(X)</span><br><span class="line"># 属性explained_variance，查看降维后每个新特征向量上所带的信息量大小(可解释性方差的大小) </span><br><span class="line">pca.explained_variance_</span><br><span class="line"># 属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比 </span><br><span class="line"># 又叫做可解释方差贡献率</span><br><span class="line">pca.explained_variance_ratio_</span><br><span class="line"># 降维后的总方差为原来方差的百分比</span><br><span class="line">pca.explained_variance_ratio_.sum()</span><br></pre></td></tr></table></figure>
<p> 参数：</p>
<ul>
<li>n_components: n_components是我们降维后需要的维度，即降维后需要保留的特征数量，降维流程中第二步里需要确认的k值，一般输入[0, min(X.shape)]范围中的整数。</li>
<li>svd_solver: 奇异值分解器。sklearn将降 维流程拆成了两部分:一部分是计算特征空间V，由奇异值分解完成，另一部分是映射数据和求解新特征矩阵，由主成分分析完成，实现了用SVD的性质减少计算量，却让信息量的评估指标是方差，具体流程如下图:</li>
<li>random_state<br>接口：</li>
<li>inverse_transform </li>
</ul>
</li>
<li><p>选择最好的n_components:</p>
<ul>
<li><p>方法一：累积可解释方差贡献率曲线，选择拐点作为最优参数值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">pca_line = PCA().fit(X) plt.plot([1,2,3,4],np.cumsum(pca_line.explained_variance_ratio_)) </span><br><span class="line">plt.xticks([1,2,3,4]) #这是为了限制坐标轴显示为整数</span><br><span class="line">plt.xlabel(&quot;number of components after dimension reduction&quot;) </span><br><span class="line">plt.ylabel(&quot;cumulative explained variance&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>方法二：最大似然估计自选超参数法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pca_mle = PCA(n_components=&quot;mle&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>方法三：按信息量占比选超参数<br>输入[0,1]之间的浮点数，并且让参数svd_solver ==’full’，表示希望降维后的总解释性方差占比大于n_components 指定的百分比，即是说，希望保留百分之多少的信息量。比如说，如果我们希望保留97%的信息量，就可以输入 n_components = 0.97，PCA会自动选出能够让保留的信息量超过97%的特征数量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pca_f = PCA(n_components=0.97,svd_solver=&quot;full&quot;)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://angelteng.github.io/blog/blog/2019/05/27/特征工程-embedded嵌入法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Angel Teng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://angelteng.github.io/blog/images/angel.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Code war of Angel">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/blog/2019/05/27/特征工程-embedded嵌入法/" itemprop="url">特征工程_embedded嵌入法与包装法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-27T11:19:56+08:00">
                2019-05-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="嵌入法"><a href="#嵌入法" class="headerlink" title="嵌入法"></a>嵌入法</h1><ol>
<li><p>嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。并且，由于考虑特征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版。<br><img src="0.png" alt="image.png"></p>
</li>
<li><p>sklearn.feature_selection.SelectFromModel (estimator, threshold=None, prefit=False, norm_order=1,<br>max_features=None)</p>
<ul>
<li>estimator　　　　　　　　<br>  使用的模型评估器，只要是带feature_importances_或者coef_属性，或带有l1和l2惩罚项的模型都可以使用</li>
<li>threshold<br>  特征重要性的阈值，重要性低于这个阈值的特征都将被删除</li>
<li>prefit<br>  默认False，判断是否将实例化后的模型直接传递给构造函数。如果为True，则必须直接调用fit和transform，不能使用fit_transform，并且SelectFromModel不能与cross_val_score，GridSearchCV和克隆估计器的类似实用程序一起使用。</li>
<li>norm_order<br>  k可输入非零整数，正无穷，负无穷，默认值为1<br>  在评估器的coef_属性高于一维的情况下，用于过滤低于阈值的系数的向量的范数的阶数</li>
<li><p>max_features<br>  在阈值设定下，要选择的最大特征数。要禁用阈值并仅根据max_features选择，请设置threshold = -np.inf</p>
<p>随机森林中的应用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import SelectFromModel</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier as RFC</span><br><span class="line">RFC_ = RFC(n_estimators =10,random_state=0)</span><br><span class="line">X_embedded = SelectFromModel(RFC_,threshold=0.005).fit_transform(X,y)</span><br><span class="line"># 可以使用学习曲线确认threshold最佳值</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>在嵌入法下，我们很容易就能够实现特征选择的目标：减少计算量，提升模型表现。因此，比起要思考很多统计量的过滤法来说，嵌入法可能是更有效的一种方法。然而，在算法本身很复杂的时候，过滤法的计算远远比嵌入法要快，所以大型数据中，我们还是会优先考虑过滤法。</p>
</li>
</ol>
<p>参考：<br><a href="https://www.cnblogs.com/zhange000/articles/10751525.html" target="_blank" rel="noopener">特征选择-嵌入</a></p>
<h1 id="包装法"><a href="#包装法" class="headerlink" title="包装法"></a>包装法</h1><p>包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</p>
<ol>
<li>递归特征消除法<br> 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import RFE</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">#递归特征消除法，返回特征选择后的数据</span><br><span class="line">#参数estimator为基模型</span><br><span class="line">#参数n_features_to_select为选择的特征个数</span><br><span class="line">RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data,iris.target)</span><br></pre></td></tr></table></figure>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://angelteng.github.io/blog/blog/2019/05/24/特征工程_Filter过滤法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Angel Teng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://angelteng.github.io/blog/images/angel.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Code war of Angel">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/blog/2019/05/24/特征工程_Filter过滤法/" itemprop="url">特征工程_Filter过滤法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-24T17:36:09+08:00">
                2019-05-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li><p>流程</p>
<ul>
<li>特征提取</li>
<li>特征创造</li>
<li>特征选择：过滤-&gt;嵌入/包装/降维</li>
</ul>
</li>
<li><p>Filter过滤法</p>
<ol>
<li><p>方差过滤<br> 这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无论接下来的特征工程要做什么，都要优先消除方差为0的特征。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import VarianceThreshold</span><br><span class="line">selector = VarianceThreshold(threshold=0) #阈值，抛弃所有低于阈值的特征</span><br><span class="line">X_var0 = selector.fit_transform(X)</span><br></pre></td></tr></table></figure>
<p> 影响：最近邻算法KNN，单棵决策树，支持向量机SVM，神经网络，回归算法，都需要遍历特征或升维来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。但对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选择对它来说效果平平。</p>
</li>
<li><p>相关性过滤</p>
<ol>
<li><p>卡方过滤</p>
<ul>
<li>卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。</li>
<li><p>卡方检验类feature_selection.chi2计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。再结合feature_selection.SelectKBest这个可以输入”评分标准“来选出前K个分数最高的特征的类。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import SelectKBest</span><br><span class="line">from sklearn.feature_selection import chi2</span><br><span class="line">#假设需要300个特征</span><br><span class="line">X_fschi = SelectKBest(chi2, k=300).fit_transform(X_fsvar, y)</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义超参数K的值</p>
<ul>
<li>学习曲线</li>
<li><p>看p值选择K<br>  卡方检验的本质是推测两组数据之间的差异，其检验的原假设是”两组数据是相互独立的”。卡方检验返回卡方值和P值两个统计量，其中卡方值很难界定有效的范围，而p值，我们一般使用0.01或0.05作为显著性水平</p>
<ul>
<li>P&lt;=0.05/0.01: 拒绝原假设，接受备择假设 </li>
<li><p>p&gt;0.05/0.01 : 接受原假设</p>
<p>从特征工程的角度，我们希望选取卡方值很大，p值小于0.05的特征，即和标签是相关联的特征。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chivalue, pvalues_chi = chi2(X_fsvar,y)</span><br><span class="line"># k取多少？我们想要消除所有p值大于设定值，比如0.05或0.01的特征：</span><br><span class="line">k_ = chivalue.shape[0] - (pvalues_chi &gt; 0.05).sum()</span><br><span class="line">X_fschi = SelectKBest(chi2, k=k_).fit_transform(X_fsvar, y)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>F检验</p>
<ul>
<li>是用来捕捉每个特征与标签之间的线性关系的过滤方法。它即可以做回归(feature_selection.f_regression)也可以做分类(feature_selection.f_classif)。</li>
<li>和卡方检验一样，这两个类需要和类SelectKBest连用.</li>
<li>F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我们会先将数据转换成服从正态分布的方式。</li>
<li>F检验的本质是寻找两组数据之间的线性关系，其原假设是”数据不存在显著的线性关系“。它返回F值和p值两个统计量。和卡方过滤一样，我们希望选取p值小于0.05或0.01的特征，这些特征与标签时显著线性相关的。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import f_classif</span><br><span class="line">F, pvalues_f = f_classif(X_fsvar,y)</span><br><span class="line">k_ = F.shape[0] - (pvalues_f &gt; 0.05).sum()</span><br><span class="line">X_fsF = SelectKBest(f_classif, k=k_).fit_transform(X_fsvar, y)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>互信息法</p>
<ul>
<li>互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。和F检验相似，它既可以做回归也可以做分类，并且包含两个类feature_selection.mutual_info_classif（互信息分类）和feature_selection.mutual_info_regression（互信息回归）。</li>
<li>互信息法不返回p值或F值类似的统计量，它返回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间取值，为0则表示两个变量独立，为1则表示两个变量完全相关。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import mutual_info_classif as MIC</span><br><span class="line">result = MIC(X_fsvar,y)</span><br><span class="line">k_ = result.shape[0] - sum(result &lt;= 0)</span><br><span class="line">X_fsmic = SelectKBest(MIC, k=k_).fit_transform(X_fsvar, y)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
<li><p>总结</p>
</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th>说明</th>
<th>超参数配置</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">VarianceThreshold</td>
<td>方差过滤，可输入方差阈值，返回方差大于阈值的新特征矩阵</td>
<td>一般使用0/1筛选，可以画学习曲线</td>
</tr>
<tr>
<td style="text-align:left">SelectKBest</td>
<td>用来选取K个统计量结果最佳的矩阵，生成符合统计量要求的新特征矩阵</td>
<td>看配合使用的统计量</td>
</tr>
<tr>
<td style="text-align:left">chi2</td>
<td>卡方验证，专用于分类算法</td>
<td>追求p小于显著性水平的特征</td>
</tr>
<tr>
<td style="text-align:left">f_classif</td>
<td>F检验分类，只能捕捉线性相关，要求数据服从正态分布</td>
<td>追求p小于显著性水平的特征</td>
</tr>
<tr>
<td style="text-align:left">f_regression</td>
<td>F检验回归，只能捕捉线性相关，要求数据服从正态分布</td>
<td>追求p小于显著性水平的特征</td>
</tr>
<tr>
<td style="text-align:left">mutual_info_classif</td>
<td>互信息分类，可以捕捉任何相关性，不能用于稀疏矩阵</td>
<td>追求互信息大于0的特征</td>
</tr>
<tr>
<td style="text-align:left">mutual_info_regression</td>
<td>互信息回归，可以捕捉任何相关性，不能用于稀疏矩阵</td>
<td>追求互信息大于0的特征</td>
</tr>
</tbody>
</table>
<p> 参考：<br><a href="https://www.cnblogs.com/zhange000/articles/10750903.html" target="_blank" rel="noopener">Filter过滤法</a> </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://angelteng.github.io/blog/blog/2019/05/24/数据预处理/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Angel Teng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://angelteng.github.io/blog/images/angel.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Code war of Angel">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/blog/2019/05/24/数据预处理/" itemprop="url">数据预处理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-24T14:51:48+08:00">
                2019-05-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li><p>数据挖掘的流程</p>
<ul>
<li>获取数据</li>
<li>数据预处理：从数据中检测，纠正或删除损坏，不准确或不适用于模型的记录过程。</li>
<li>特征工程：特征工程是将原始数据转换为更能代表预测模型的潜在问题的特征的过程，可以通过挑选最相关的特征，提取特征以及创造特征来实现。</li>
<li>建模</li>
<li>上线、验证</li>
</ul>
</li>
<li><p>数据无量纲化<br>在机器学习算法实践中，我们往往有着将不同规格的数据转换到同一规格，或不同分布的数据转换到某个特定分布 的需求，这种需求统称为将数据“无量纲化”。</p>
<ul>
<li>线性的无量纲化<ul>
<li>中心化</li>
<li>缩放</li>
</ul>
</li>
<li>非线性的无量纲化</li>
</ul>
<ol>
<li><p>数据归一化（异常值敏感）<br> 当数据(x)按照最小值中心化后，再按极差(最大值 - 最小值)缩放，数据移动了最小值个单位，并且会被收敛到 [0,1]之间。<br> <img src="0.png" alt="image.png"></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">scaler = MinMaxScaler(feature_range=[5,10])   # scaler后5~10之间，默认 0~1</span><br><span class="line">result = scaler.fit(data)</span><br><span class="line">result = scaler.transform(data)</span><br><span class="line"># 一步达成</span><br><span class="line">result_ = scaler.fit_transform(data)</span><br><span class="line"># 逆转</span><br><span class="line">scaler.inverse_transform(result)</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据标准化(优先选用)<br> 当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布(即标准正态分布)<br> <img src="1.png" alt="image.png"></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(data)</span><br><span class="line">print(scaler.mean_)</span><br><span class="line">print(scaler.var_)</span><br><span class="line">res = scaler.transform(data)</span><br><span class="line">res.mean()</span><br><span class="line">res.std()</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
<pre><code>![image.png](2.png)
</code></pre><ol start="3">
<li><p>处理缺失值</p>
<ol>
<li><p>impute.SimpleImputer(missing_values=nan, strategy=’mean’, fill_value=None, verbose=0, copy=True)</p>
<ul>
<li>missing_values ：缺失数据的值</li>
<li>strategy：填补策略<ul>
<li>mean：均值填补(仅对数值型特征可用) </li>
<li>median：中值填补(仅对数值型特征可用) </li>
<li>most_frequent：众数填补(对数值型和字符型特征都可用) </li>
<li>constant：参考参数“fill_value”中的值(对数值型和字符型特征都可用)</li>
</ul>
</li>
<li>fill_value：填补值</li>
<li><p>copy： 创建副本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.impute import SimpleImputer</span><br><span class="line">im = SimpleImputer(strategy=&apos;median&apos;)</span><br><span class="line">im.fit(age)</span><br><span class="line">im.transform(age)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>使用pandas处理</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.loc[:,&quot;Age&quot;] = data.loc[:,&quot;Age&quot;].fillna(data.loc[:,&quot;Age&quot;].median())</span><br><span class="line">data.dropna(axis=0,inplace=True) #删除行</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>处理分类型特征<br> 编码： 文字型数据转化为数字型数据。<br> 哑变量：它是人为虚设的变量，来反映某个变量的相互独立的不同属性。（比如身份：学生、工人、教师）</p>
<ol>
<li><p>preprocessing.LabelEncoder: 标签y专用，将分类转化为分类数值</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import LabelEncoder</span><br><span class="line">y = data.Survived</span><br><span class="line">le = LabelEncoder()</span><br><span class="line">data.Survived=le.fit_transform(y)</span><br></pre></td></tr></table></figure>
</li>
<li><p>preprocessing.LabelBinarizer: 标签y专用，将分类转化为哑变量</p>
</li>
<li><p>preprocessing.OrdinalEncoder: 特征X专用，将分类特征转化为分类数值 (如果特征的值有关联性，处理有序变量)</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import OrdinalEncoder</span><br><span class="line">x = data.Embarked</span><br><span class="line">le = OrdinalEncoder()</span><br><span class="line">data.Embarked=le.fit_transform(x.values.reshape(-1,1))</span><br></pre></td></tr></table></figure>
</li>
<li><p>preprocessing.OneHotEncoder: 独热编码，创建哑变量 （特征的值无关关联性，处理名义变量）<br> <img src="3.png" alt="image.png"></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import OneHotEncoder</span><br><span class="line">xs = data.loc[:,[&apos;Sex&apos;,&apos;Embarked&apos;]]</span><br><span class="line">one= OneHotEncoder()</span><br><span class="line">res= one.fit(xs)</span><br><span class="line">columns = res.get_feature_names() #查看类型名称</span><br><span class="line">res= one.transform(xs).toarray() </span><br><span class="line">newdata = pd.concat([data,pd.DataFrame(res,columns=columns)],axis=1)</span><br></pre></td></tr></table></figure>
<p><img src="4.png" alt="image.png"></p>
</li>
</ol>
</li>
<li><p>处理连续性变量<br> 二值化：根据阈值将数据二值化，分为0/1。<br> 分段：连续型变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。</p>
<ol>
<li><p>preprocessing.Binarizer: 根据阈值将数据二值化</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line">x = data2.Age.values.reshape(-1,1)</span><br><span class="line">bins = Binarizer(threshold=30) # 阈值=30</span><br><span class="line">res = bins.fit_transform(x)</span><br></pre></td></tr></table></figure>
</li>
<li><p>preprocessing.KBinsDiscretizer: 将连续性变量排序后分箱<br> 参数：</p>
<pre><code>- n_bins: 每个特征中分箱的个数。默认5。
- encode: 编码方式
    - onehot: 哑变量，返回一个稀疏数组
    - ordinal: 每个特征中每个箱都被编码为一个整数
    - onehot-dense: 哑变量，返回密集数组
- strategy: 定义箱框的方式
    - uniform: 等宽分箱
    - quantile: 等位分箱，每个箱内样本数量相同（默认）
    - kmeans: 按聚类分箱
</code></pre> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import KBinsDiscretizer</span><br><span class="line">X = data.Age.values.reshape(-1,1)</span><br><span class="line">est = KBinsDiscretizer(n_bins=3, encode=&apos;ordinal&apos;, strategy=&apos;uniform&apos;)</span><br><span class="line">est.fit_transform(X)</span><br><span class="line">#查看转换后分的箱:变成了一列中的三箱 set(est.fit_transform(X).ravel())</span><br><span class="line">est = KBinsDiscretizer(n_bins=3, encode=&apos;onehot&apos;, strategy=&apos;uniform&apos;) #查看转换后分的箱:变成了哑变量</span><br><span class="line">est.fit_transform(X).toarray()</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://angelteng.github.io/blog/blog/2019/05/24/随机森林/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Angel Teng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://angelteng.github.io/blog/images/angel.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Code war of Angel">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/blog/2019/05/24/随机森林/" itemprop="url">随机森林</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-24T10:59:56+08:00">
                2019-05-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>随机森林是一种集成算法，集成算法的目标：考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或<br>分类表现。</li>
<li>多个模型集成成为的模型叫做集成评估器(ensemble estimator)，组成集成评估器的每个模型都叫做基评估器 (base estimator)。通常来说，有三类集成算法:装袋法(Bagging)，提升法(Boosting)和stacking。</li>
<li><p>随机森林是一种装袋法模型，装袋法的核心是构建多个独立的评估器，然后对其预测进行平均或多数表决原则决定集成评估器的效果。而提升法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本 进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树。</p>
</li>
<li><p>随机森林的特点：</p>
<ul>
<li>在与其它现有的算法相比，其预测准确率很好</li>
<li>在较大的数据集上计算速度依然很快</li>
<li>不需要降维，算法本身是采取随机降维的</li>
<li>他能处理有缺失值的数据集。算法内部有补缺失值的函数</li>
<li>能给出变量的重要性</li>
<li>能处理imbalanced data set</li>
<li>能给出观测实例间的相似度矩阵，其实就是proximity啦，继而能做clustering 和 location outlier</li>
<li>能对unlabeled data 进行无监督的学习，进行clustering</li>
<li>生成的森林可以保留，应用在新的数据集上</li>
</ul>
</li>
<li><p>控制评估器的参数</p>
<ul>
<li>criterion: 不纯度衡量指标</li>
<li>max_depth: 树最大深度</li>
<li>min_samples_leaf：一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本</li>
<li>min_samples_split: 一个分支至少含有min_samples_split个训练样本</li>
<li>max_features： 限制分支时考虑特征个数</li>
<li>min_impurity_decrease: 限制信息增益大小</li>
</ul>
</li>
</ol>
<p>重要参数</p>
<pre><code>- n_estimators： 这是森林中树木的数量，即基基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators 越大，模型的效往往越好。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林 的精确性往往不在上升或开始波动。
</code></pre><p>重要属性</p>
<pre><code>- .estimators_是用来查看随机森林中所有树的列表的。
- oob_score_指的是袋外得分。随机森林为了确保林中的每棵树都不尽相同，所以采用了对训练集进行有放回抽样的 方式来不断组成信的训练集，在这个程中，会有一些数据从来没有被随机挑选到，他们就被叫做“袋外数据”。
- .feature_importances_和决策树中的.feature_importances_用法和含义都一致，是返回特征的重要性。
- predict_proba()： 返回每个测试样本对应的被分到每一类标签的概率
</code></pre><ol start="6">
<li><p>在sklearn中的使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</li>
<li><p>泛化误差<br><img src="0.png" alt="image.png"><br>当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力就不够，所以误差也会大。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。<br>树模型是天生位于图的右上角的模型，随机森林是以树模型为基础，所以随机森林也是天生复杂度高的模型。随机森林的参数，都是向着一个目标去:减少模型的复杂度，把模型往图像的左边移动，防止过拟合。</p>
</li>
<li><p>参数对误差的影响</p>
</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">影响</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">n_estimators</td>
<td style="text-align:left">提升至平稳，n_estimators↑，不影响单个模型的复杂度</td>
</tr>
<tr>
<td style="text-align:left">max_depth</td>
<td style="text-align:left">有增有减，默认最大深度，即最高复杂度</td>
</tr>
<tr>
<td style="text-align:left">min_samples_leaf</td>
<td style="text-align:left">有增有减，默认最小限制1，即最高复杂度</td>
</tr>
<tr>
<td style="text-align:left">min_samples_split</td>
<td style="text-align:left">有增有减，默认最小限制2，即最高复杂度</td>
</tr>
<tr>
<td style="text-align:left">max_features</td>
<td style="text-align:left">有增有减，默认auto，是特征总数的开平方，位于中间复杂度，max_features↓，模型更简单</td>
</tr>
<tr>
<td style="text-align:left">criterion</td>
<td style="text-align:left">有增有减，一般使用gini</td>
</tr>
</tbody>
</table>
<ol start="9">
<li><p>调参</p>
<ul>
<li><p>学习曲线</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">scorel = []</span><br><span class="line">for i in range(0,200,10):</span><br><span class="line">    rfc = RandomForestClassifier(n_estimators=i+1,</span><br><span class="line">                                n_jobs=-1,</span><br><span class="line">                                random_state=90)</span><br><span class="line">    score = cross_val_score(rfc,data.data,data.target,cv=10).mean()</span><br><span class="line">    scorel.append(score)</span><br><span class="line">print(max(scorel),(scorel.index(max(scorel))*10)+1)</span><br><span class="line">plt.figure(figsize=[20,5])</span><br><span class="line">plt.plot(range(1,201,10),scorel)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>网格搜索</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line">param_grid = &#123;&apos;max_depth&apos;:np.arange(1,20,1)&#125;</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=39</span><br><span class="line">                     ,random_state=90</span><br><span class="line">                    )</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=10)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line">GS.best_params_</span><br><span class="line">GS.best_score_</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<p>参考：<br><a href="http://edu.cda.cn/course/982/tasks" target="_blank" rel="noopener">菜菜的机器学习sklearn课堂</a><br><a href="http://www.cnblogs.com/litao1105/p/5021747.html" target="_blank" rel="noopener">随机森林算法－Deep Dive</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://angelteng.github.io/blog/blog/2019/05/23/决策树/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Angel Teng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://angelteng.github.io/blog/images/angel.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Code war of Angel">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/blog/2019/05/23/决策树/" itemprop="url">决策树</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-23T17:55:30+08:00">
                2019-05-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>决策树算法的核心是要解决两个问题: <ul>
<li>如何从数据表中找出最佳节点和最佳分枝? </li>
<li>如何让决策树停止生长，防止过拟合?</li>
</ul>
</li>
<li>流程：<ul>
<li>计算全部特征的不纯度</li>
<li>选取不纯度指标最优的特征来分化</li>
<li>在第一个特征分支下，计算全部特征不纯度</li>
<li>选取不纯度指标最优的特征继续分化</li>
</ul>
</li>
<li><p>sklearn中使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import tree</span><br><span class="line">tree.DecisionTreeClassifier:分类树</span><br><span class="line">tree.DecisionTreeRegressor:回归树</span><br><span class="line">tree.export_graphviz:将生成的决策树导出为DOT格式，画图专用</span><br><span class="line">tree.ExtraTreeClassifier:高随机版本的分类树</span><br><span class="line">tree.ExtraTreeRegressor:高随机版本的回归树</span><br></pre></td></tr></table></figure>
</li>
<li><p>重要参数</p>
<ul>
<li>criterion: 不纯度的计算方法（不纯度越低、拟合效果越好）<ul>
<li>entropy 信息熵</li>
<li>gini    基尼系数（默认）</li>
</ul>
</li>
<li>random_state: 随机模式，高纬度下随机性更明显<ul>
<li>None （默认）</li>
<li>任意整数</li>
</ul>
</li>
<li>splitter： 随机选项<ul>
<li>best： 优先选择更重要的特征进行分支</li>
<li>random： 分支随机（防止过拟合）</li>
</ul>
</li>
</ul>
</li>
<li>剪枝参数<br>在不加限制的情况下，一棵决策树会生长到衡量不纯度的指标最优，或者没有更多的特征可用为止，这样的决策树往往会过拟合。<ul>
<li>max_depth: 限制最大深度</li>
<li>min_samples_leaf：一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本</li>
<li>min_samples_split: 一个分支至少含有min_samples_split个训练样本</li>
<li>max_features: 限制分枝时考虑的特征个数 </li>
<li>min_impurity_decrease: 限制信息增益的大小</li>
</ul>
</li>
<li>目标权重参数<ul>
<li>class_weight：均衡标签样本的权重</li>
<li>min_ weight_fraction_leaf：基于权重的剪枝参数</li>
</ul>
</li>
<li>确定最优参数：<ul>
<li>学习曲线</li>
</ul>
</li>
</ol>
<p>参考<br><a href="http://edu.cda.cn/course/982/tasks" target="_blank" rel="noopener">菜菜的机器学习sklearn课堂</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/blog/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/blog/">1</a><span class="page-number current">2</span><a class="page-number" href="/blog/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/8/">8</a><a class="extend next" rel="next" href="/blog/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://angelteng.github.io/blog/images/angel.jpg" alt="Angel Teng">
            
              <p class="site-author-name" itemprop="name">Angel Teng</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/blog/archives/">
              
                  <span class="site-state-item-count">74</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/blog/category/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/blog/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Angel Teng</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/blog/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
