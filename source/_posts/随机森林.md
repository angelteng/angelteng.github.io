---
title: 随机森林
date: 2019-05-24 10:59:56
tags:
- 机器学习
categories: 机器学习
---
1. 随机森林是一种集成算法，集成算法的目标：考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或
分类表现。
2. 多个模型集成成为的模型叫做集成评估器(ensemble estimator)，组成集成评估器的每个模型都叫做基评估器 (base estimator)。通常来说，有三类集成算法:装袋法(Bagging)，提升法(Boosting)和stacking。
3. 随机森林是一种装袋法模型，装袋法的核心是构建多个独立的评估器，然后对其预测进行平均或多数表决原则决定集成评估器的效果。而提升法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本 进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树。

4. 随机森林的特点：
    - 在与其它现有的算法相比，其预测准确率很好
    - 在较大的数据集上计算速度依然很快
    - 不需要降维，算法本身是采取随机降维的
    - 他能处理有缺失值的数据集。算法内部有补缺失值的函数
    - 能给出变量的重要性
    - 能处理imbalanced data set
    - 能给出观测实例间的相似度矩阵，其实就是proximity啦，继而能做clustering 和 location outlier
    - 能对unlabeled data 进行无监督的学习，进行clustering
    - 生成的森林可以保留，应用在新的数据集上
    
5. 
控制评估器的参数
    - criterion: 不纯度衡量指标
    - max_depth: 树最大深度
    - min_samples_leaf：一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本
    - min_samples_split: 一个分支至少含有min_samples_split个训练样本
    - max_features： 限制分支时考虑特征个数
    - min_impurity_decrease: 限制信息增益大小

重要参数
    - n_estimators： 这是森林中树木的数量，即基基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators 越大，模型的效往往越好。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林 的精确性往往不在上升或开始波动。
    
重要属性
    - .estimators_是用来查看随机森林中所有树的列表的。
    - oob_score_指的是袋外得分。随机森林为了确保林中的每棵树都不尽相同，所以采用了对训练集进行有放回抽样的 方式来不断组成信的训练集，在这个程中，会有一些数据从来没有被随机挑选到，他们就被叫做“袋外数据”。
    - .feature_importances_和决策树中的.feature_importances_用法和含义都一致，是返回特征的重要性。
    - predict_proba()： 返回每个测试样本对应的被分到每一类标签的概率


6. 在sklearn中的使用
```
    from sklearn.ensemble import RandomForestClassifier
    ...
```

7. 泛化误差
![image.png](0.png)
当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力就不够，所以误差也会大。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。
树模型是天生位于图的右上角的模型，随机森林是以树模型为基础，所以随机森林也是天生复杂度高的模型。随机森林的参数，都是向着一个目标去:减少模型的复杂度，把模型往图像的左边移动，防止过拟合。

8. 参数对误差的影响

|参数|影响|
|:--|:--|
|n_estimators|提升至平稳，n_estimators↑，不影响单个模型的复杂度|
|max_depth|有增有减，默认最大深度，即最高复杂度|
|min_samples_leaf|有增有减，默认最小限制1，即最高复杂度|
|min_samples_split|有增有减，默认最小限制2，即最高复杂度|
|max_features|有增有减，默认auto，是特征总数的开平方，位于中间复杂度，max_features↓，模型更简单|
|criterion|有增有减，一般使用gini|

9. 调参
    - 学习曲线
    ```
        from sklearn.model_selection import cross_val_score
        import matplotlib.pyplot as plt
        scorel = []
        for i in range(0,200,10):
            rfc = RandomForestClassifier(n_estimators=i+1,
                                        n_jobs=-1,
                                        random_state=90)
            score = cross_val_score(rfc,data.data,data.target,cv=10).mean()
            scorel.append(score)
        print(max(scorel),(scorel.index(max(scorel))*10)+1)
        plt.figure(figsize=[20,5])
        plt.plot(range(1,201,10),scorel)
        plt.show()
    ```
    - 网格搜索
    ```
        from sklearn.model_selection import GridSearchCV
        param_grid = {'max_depth':np.arange(1,20,1)}
        rfc = RandomForestClassifier(n_estimators=39
                             ,random_state=90
                            )
        GS = GridSearchCV(rfc,param_grid,cv=10)
        GS.fit(data.data,data.target)
        GS.best_params_
        GS.best_score_
    ```


参考：
[菜菜的机器学习sklearn课堂](http://edu.cda.cn/course/982/tasks)
[随机森林算法－Deep Dive](http://www.cnblogs.com/litao1105/p/5021747.html)
